#
# For licensing see accompanying LICENSE file.
# Copyright (C) 2024 Apple Inc. All Rights Reserved.
#

import torch
from lpips import LPIPS
import torch.nn as nn
import torch.nn.functional as F

from hugs.utils.sampler import PatchSampler

from .utils import l1_loss, ssim

# å®ƒç»“åˆäº†å›¾åƒé‡å»ºæŸå¤±ï¼ˆL1ã€SSIMã€LPIPSï¼‰ã€äººä½“å‡ ä½•ç›‘ç£ï¼ˆLBS æƒé‡å›žå½’ï¼‰ã€ä»¥åŠäºº/åœºæ™¯åˆ†ç¦»ç›‘ç£ç­‰ã€‚
class HumanSceneLoss(nn.Module):
    def __init__(
        self,
        l_ssim_w=0.2,
        l_l1_w=0.8,
        l_lpips_w=0.0,
        l_lbs_w=0.0,
        l_humansep_w=0.0,
        num_patches=4,
        patch_size=32,
        use_patches=True,
        bg_color='white',
    ):
        super(HumanSceneLoss, self).__init__()
        
        # âœ… åˆå§‹åŒ–ä¼ å…¥çš„æƒé‡é¡¹ï¼š
        # l_ssim_w, l_l1_w, l_lpips_wï¼šå›¾åƒç›¸ä¼¼æ€§æŸå¤±çš„åŠ æƒç³»æ•°ã€‚
        # l_lbs_wï¼šLBS æƒé‡çº¦æŸé¡¹çš„ç³»æ•°ã€‚        
        # l_humansep_wï¼šåœ¨ human_scene æ¨¡å¼ä¸‹ç”¨äºŽåˆ†ç¦»äººä½“çš„é¢å¤–æŸå¤±åŠ æƒ
        # num_patches, patch_size, use_patchesï¼šæ˜¯å¦ä½¿ç”¨ PatchSampler æ¥è¿›è¡Œ LPIPS è®¡ç®—ã€‚
        self.l_ssim_w = l_ssim_w
        self.l_l1_w = l_l1_w
        self.l_lpips_w = l_lpips_w
        self.l_lbs_w = l_lbs_w
        self.l_humansep_w = l_humansep_w
        self.use_patches = use_patches
        
        self.bg_color = bg_color
        self.lpips = LPIPS(net="vgg", pretrained=True).to('cuda')
    
        for param in self.lpips.parameters(): param.requires_grad=False
        
        if self.use_patches:
            self.patch_sampler = PatchSampler(num_patch=num_patches, patch_size=patch_size, ratio_mask=0.9, dilate=0)
    
    # data: GT å›¾åƒã€maskã€bbox ç­‰ã€‚
    # render_pkg: æ¸²æŸ“ç»“æžœï¼ˆå®Œæ•´å›¾åƒ / åˆ†ç¦»å›¾åƒç­‰ï¼‰
    # human_gs_out: é«˜æ–¯è¾“å‡ºï¼ˆå« LBS æƒé‡ï¼‰ã€‚
    # render_mode: å½“å‰æ¸²æŸ“ç±»åž‹ï¼šhuman / scene / human_sceneã€‚
    def forward(
        self, 
        data, 
        render_pkg,
        human_gs_out,
        render_mode, 
        human_gs_init_values=None,
        bg_color=None,
        human_bg_color=None,
    ):
        loss_dict = {}
        extras_dict = {}
        
        if bg_color is not None:
            self.bg_color = bg_color
            
        if human_bg_color is None:
            human_bg_color = self.bg_color
            
        gt_image = data['rgb']
        mask = data['mask'].unsqueeze(0)
        
        pred_img = render_pkg['render']

        #         | æ¸²æŸ“æ¨¡å¼            | æ“ä½œè¯´æ˜Ž                                              |
        # | --------------- | ------------------------------------------------- |
        # | `'human'`       | `gt = rgb Ã— mask + bg_color Ã— (1 - mask)`ï¼Œåªç›‘ç£äººä½“åŒºåŸŸ |
        # | `'scene'`       | `gt = rgb Ã— (1 - mask)`ï¼Œåªç›‘ç£åœºæ™¯åŒºåŸŸ                   |
        # | `'human_scene'` | ä½¿ç”¨åŽŸå›¾æ•´ä½“è¿›è¡Œç›‘ç£                                        |
        if render_mode == "human":
            gt_image = gt_image * mask + human_bg_color[:, None, None] * (1. - mask)
            extras_dict['gt_img'] = gt_image
            extras_dict['pred_img'] = pred_img
        elif render_mode == "scene":
            # invert the mask
            extras_dict['pred_img'] = pred_img
            
            mask = (1. - data['mask'].unsqueeze(0))
            gt_image = gt_image * mask
            pred_img = pred_img * mask
            
            extras_dict['gt_img'] = gt_image
        else:
            extras_dict['gt_img'] = gt_image
            extras_dict['pred_img'] = pred_img
        
        if self.l_l1_w > 0.0:
            if render_mode == "human":
                Ll1 = l1_loss(pred_img, gt_image, mask)
            elif render_mode == "scene":
                Ll1 = l1_loss(pred_img, gt_image, 1 - mask)
            elif render_mode == "human_scene":
                Ll1 = l1_loss(pred_img, gt_image)
            else:
                raise NotImplementedError
            loss_dict['l1'] = self.l_l1_w * Ll1

        # ç»“æž„ç›¸ä¼¼æ€§
        if self.l_ssim_w > 0.0:
            loss_ssim = 1.0 - ssim(pred_img, gt_image)
            if render_mode == "human":
                loss_ssim = loss_ssim * (mask.sum() / (pred_img.shape[-1] * pred_img.shape[-2]))
            elif render_mode == "scene":
                loss_ssim = loss_ssim * ((1 - mask).sum() / (pred_img.shape[-1] * pred_img.shape[-2]))
            elif render_mode == "human_scene":
                loss_ssim = loss_ssim
                
            loss_dict['ssim'] = self.l_ssim_w * loss_ssim

        # ä½œç”¨ï¼šæ„ŸçŸ¥å±‚çº§ä¸Šæ¯”è¾ƒé¢„æµ‹ä¸Ž GT å›¾åƒæ˜¯å¦ä¸€è‡´ï¼ˆç”¨ VGG ç‰¹å¾ï¼‰ã€‚
        if self.l_lpips_w > 0.0 and not render_mode == "scene":
            if self.use_patches:
                if render_mode == "human":
                    bg_color_lpips = torch.rand_like(pred_img)
                    image_bg = pred_img * mask + bg_color_lpips * (1. - mask)
                    gt_image_bg = gt_image * mask + bg_color_lpips * (1. - mask)
                    _, pred_patches, gt_patches = self.patch_sampler.sample(mask, image_bg, gt_image_bg)
                else:
                    _, pred_patches, gt_patches = self.patch_sampler.sample(mask, pred_img, gt_image)
                    
                loss_lpips = self.lpips(pred_patches.clip(max=1), gt_patches).mean()
                loss_dict['lpips_patch'] = self.l_lpips_w * loss_lpips
            else:
                bbox = data['bbox'].to(int)
                cropped_gt_image = gt_image[:, bbox[0]:bbox[2], bbox[1]:bbox[3]]
                cropped_pred_img = pred_img[:, bbox[0]:bbox[2], bbox[1]:bbox[3]]
                loss_lpips = self.lpips(cropped_pred_img.clip(max=1), cropped_gt_image).mean()
                loss_dict['lpips'] = self.l_lpips_w * loss_lpips

        # ðŸ§â€â™‚ï¸ 5. äººä½“åŒºåŸŸä¸“å±žç›‘ç£ï¼ˆhuman_scene æ¨¡å¼ï¼‰
        if self.l_humansep_w > 0.0 and render_mode == "human_scene":
            pred_human_img = render_pkg['human_img']
            gt_human_image = gt_image * mask + human_bg_color[:, None, None] * (1. - mask)
            
            Ll1_human = l1_loss(pred_human_img, gt_human_image, mask)
            loss_dict['l1_human'] = self.l_l1_w * Ll1_human * self.l_humansep_w
            
            loss_ssim_human = 1.0 - ssim(pred_human_img, gt_human_image)
            loss_ssim_human = loss_ssim_human * (mask.sum() / (pred_human_img.shape[-1] * pred_human_img.shape[-2]))
            loss_dict['ssim_human'] = self.l_ssim_w * loss_ssim_human * self.l_humansep_w
            
            bg_color_lpips = torch.rand_like(pred_human_img)
            image_bg = pred_human_img * mask + bg_color_lpips * (1. - mask)
            gt_image_bg = gt_human_image * mask + bg_color_lpips * (1. - mask)
            _, pred_patches, gt_patches = self.patch_sampler.sample(mask, image_bg, gt_image_bg)
            loss_lpips_human = self.lpips(pred_patches.clip(max=1), gt_patches).mean()
            loss_dict['lpips_patch_human'] = self.l_lpips_w * loss_lpips_human * self.l_humansep_w

        # æ¯”è¾ƒç½‘ç»œé¢„æµ‹çš„ LBS æƒé‡ vs GT æƒé‡ï¼ˆæˆ–åˆå§‹æƒé‡ï¼‰ã€‚
        # å¸¸ç”¨äºŽæ­£åˆ™åŒ–æŽ§åˆ¶ï¼Œé˜²æ­¢é«˜æ–¯ç»‘å®šåç¦»äººä½“ç»“æž„ã€‚
        if self.l_lbs_w > 0.0 and human_gs_out['lbs_weights'] is not None and not render_mode == "scene":
            if 'gt_lbs_weights' in human_gs_out.keys():
                loss_lbs = F.mse_loss(
                    human_gs_out['lbs_weights'], 
                    human_gs_out['gt_lbs_weights'].detach()).mean()
            else:
                loss_lbs = F.mse_loss(
                    human_gs_out['lbs_weights'], 
                    human_gs_init_values['lbs_weights']).mean()
            loss_dict['lbs'] = self.l_lbs_w * loss_lbs
        
        loss = 0.0
        for k, v in loss_dict.items():
            loss += v
        
        return loss, loss_dict, extras_dict
    
