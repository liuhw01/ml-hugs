#
# For licensing see accompanying LICENSE file.
# Copyright (C) 2024 Apple Inc. All Rights Reserved.
#

import math
import os
import cv2
import glob
import copy
from loguru import logger
import torch
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader

from .neuman_utils import neuman_helper
from .neuman_utils.geometry import transformations
from .neuman_utils.cameras.camera_pose import CameraPose
from .neuman_utils.geometry.basics import Translation, Rotation
from hugs.cfg.constants import AMASS_SMPLH_TO_SMPL_JOINTS, NEUMAN_PATH
from hugs.utils.graphics import get_projection_matrix, BasicPointCloud


def get_center_and_diag(cam_centers):
    cam_centers = np.vstack(cam_centers)
    avg_cam_center = np.mean(cam_centers, axis=0, keepdims=True)
    center = avg_cam_center
    dist = np.linalg.norm(cam_centers - center, axis=1, keepdims=True)
    diagonal = np.max(dist)
    return center.flatten(), diagonal


def load_smpl_param(path):
    smpl_params = dict(np.load(str(path)))
    if "thetas" in smpl_params:
        smpl_params["body_pose"] = smpl_params["thetas"][..., 3:]
        smpl_params["global_orient"] = smpl_params["thetas"][..., :3]
    return {
        "betas": smpl_params["betas"].astype(np.float32).reshape(1, 10),
        "body_pose": smpl_params["body_pose"].astype(np.float32),
        "global_orient": smpl_params["global_orient"].astype(np.float32),
        "transl": smpl_params["transl"].astype(np.float32),
    }
    

def get_data_splits(scene):
    scene_length = len(scene.captures)
    num_val = scene_length // 5
    length = int(1 / (num_val) * scene_length)
    offset = length // 2
    val_list = list(range(scene_length))[offset::length]
    train_list = list(set(range(scene_length)) - set(val_list))
    test_list = val_list[:len(val_list) // 2]
    val_list = val_list[len(val_list) // 2:]
    assert len(train_list) > 0
    assert len(test_list) > 0
    assert len(val_list) > 0    
    return train_list, val_list, test_list

# âœ… åœºæ™¯é€‚é…ç›®çš„
# è¿™ä¸ªå‡½æ•°çš„è®¾è®¡æ˜¯ä¸ºäº†ï¼š
# ç»™ä¸åŒçš„åœºæ™¯åˆ†é…ä¸åŒçš„åŠ¨ä½œåºåˆ—ï¼ˆå¦‚è·³èˆã€è·‘æ­¥ã€ç¿»æ»šç­‰ï¼‰
# æ§åˆ¶ä½¿ç”¨çš„å¸§æ•°èŒƒå›´å’Œé‡‡æ ·é€Ÿç‡
# æ–¹ä¾¿ç”¨äºé©±åŠ¨ SMPL æ¨¡å‹ç”Ÿæˆç‰¹å®šåŠ¨æ€
def mocap_path(scene_name):
    # ./data/MoSh/MPI_mosh/50027/misc_dancing_hiphop_poses.npz
    if os.path.basename(scene_name) == 'seattle': # and opt.motion_name == 'moonwalk':
        # return './data/SFU/0018/0018_Moonwalk001_poses.npz', 0, 400, 4
        # return './data/SFU/0005/0005_Stomping001_poses.npz', 0, 800, 4
        return './data/SFU/0005/0005_SideSkip001_poses.npz', 0, 800, 4
    elif os.path.basename(scene_name) == 'citron': # and opt.motion_name == 'speedvault':
        # return './data/SFU/0008/0008_ChaCha001_poses.npz', 0, 1000, 4
        return './data/MPI_mosh/00093/irish_dance_poses.npz', 0, 1000, 4
        # return './data/SFU/0012/0012_SpeedVault001_poses.npz', 0, 340, 2
        # return './data/MPI_mosh/50027/misc_dancing_hiphop_poses.npz', 0, 2000, 4
        # return './data/SFU/0017/0017_ParkourRoll001_poses.npz', 140, 500, 4
    elif os.path.basename(scene_name) == 'parkinglot': # and opt.motion_name == 'yoga':
        return './data/SFU/0005/0005_2FeetJump001_poses.npz', 0, 1200, 4
        # return './data/SFU/0008/0008_Yoga001_poses.npz', 300, 1900, 8
    elif os.path.basename(scene_name) == 'bike': # and opt.motion_name == 'jumpandroll':
        return './data/MPI_mosh/50002/misc_poses.npz', 0, 250, 1
        # return './data/SFU/0018/0018_Moonwalk001_poses.npz', 0, 600, 4
        # return './data/SFU/0012/0012_JumpAndRoll001_poses.npz', 100, 400, 3
    elif os.path.basename(scene_name) == 'jogging': # and opt.motion_name == 'cartwheel':
        return './data/SFU/0007/0007_Cartwheel001_poses.npz', 200, 1000, 8
    elif os.path.basename(scene_name) == 'lab': # and opt.motion_name == 'chacha':
        return './data/SFU/0008/0008_ChaCha001_poses.npz', 0, 1000, 4
    else:
        raise ValueError('Define new elif branch')


def alignment(scene_name, motion_name=None):
    if os.path.basename(scene_name) == 'seattle':
        manual_trans = np.array([-2.25, 1.08, 8.18])
        manual_rot = np.array([90.4, -4.2, -1]) / 180 * np.pi
        manual_scale = 1.8
    elif os.path.basename(scene_name) == 'citron':
        manual_trans = np.array([6.33, 1.7, 10.7])
        manual_rot = np.array([72.4, 168.2, -4.4]) / 180 * np.pi
        manual_scale = 2.5
    elif os.path.basename(scene_name) == 'parkinglot':
        manual_trans = np.array([-0.8, 2.35, 12.67])
        manual_rot = np.array([94, -85, -363]) / 180 * np.pi
        manual_scale = 3.0
    elif os.path.basename(scene_name) == 'bike':
        manual_trans = np.array([0.0, 0.88, 3.89])
        manual_rot = np.array([88.8, 180, 1.8]) / 180 * np.pi
        manual_scale = 1.0
    elif os.path.basename(scene_name) == 'jogging':
        manual_trans = np.array([0.0, 0.24, 0.33])
        manual_rot = np.array([95.8, -1.2, -2.2]) / 180 * np.pi
        manual_scale = 0.25
    elif os.path.basename(scene_name) == 'lab':
        manual_trans = np.array([5.76, 3.03, 11.69])
        manual_rot = np.array([90.4, -4.2, -1.8]) / 180 * np.pi
        manual_scale = 3.0
    else:
        manual_trans = np.array([0, 0, 0])
        manual_rot = np.array([0, 0, 0]) / 180 * np.pi
        manual_scale = 1
    return manual_trans, manual_rot, manual_scale


def rendering_caps(scene_name, nframes, scene):
    if os.path.basename(scene_name) == 'seattle':
        dummy_caps = []
        for i in range(nframes):
            temp = copy.deepcopy(scene.captures[20])
            ellipse_a = 0.15 * 10
            ellipse_b = 0.05 #* 1
            x_offset= temp.cam_pose.right * ellipse_a * np.cos(i/nframes * 2 * np.pi)
            y_offset= temp.cam_pose.up * ellipse_b * np.sin(i/nframes * 2 * np.pi)
            temp.cam_pose.camera_center_in_world = temp.cam_pose.camera_center_in_world + x_offset + y_offset
            dummy_caps.append(temp)
    elif os.path.basename(scene_name) == 'citron':
        dummy_caps = []
        for i in range(nframes):
            temp = copy.deepcopy(scene.captures[33])
            ellipse_a = 0.15 * 3
            ellipse_b = 0.03 * 3 
            x_offset= temp.cam_pose.right * (ellipse_a * np.cos(2 * i/nframes * 2 * np.pi) + 0.2)
            y_offset= temp.cam_pose.up * ellipse_b * np.sin(2 * i/nframes * 2 * np.pi)
            temp.cam_pose.camera_center_in_world = temp.cam_pose.camera_center_in_world + x_offset + y_offset
            dummy_caps.append(temp)
    elif os.path.basename(scene_name) == 'parkinglot':
        dummy_caps = []
        for i in range(nframes):
            temp = copy.deepcopy(scene.captures[23])
            ellipse_a = 0.15 * 10
            ellipse_b = 0.03 * 5
            x_offset= temp.cam_pose.right * (ellipse_a * np.cos(2 * i/nframes * 2 * np.pi) + 0.2)
            y_offset= temp.cam_pose.up * ellipse_b * np.sin(2 * i/nframes * 2 * np.pi)
            temp.cam_pose.camera_center_in_world = temp.cam_pose.camera_center_in_world + x_offset + y_offset
            dummy_caps.append(temp)
    elif os.path.basename(scene_name) == 'bike':
        dummy_caps = []
        start_id = 25
        interval = 0.005 * 2
        for i in range(nframes):
            temp = copy.deepcopy(scene.captures[start_id])
            temp.cam_pose.camera_center_in_world += interval * i * temp.cam_pose.right
            dummy_caps.append(temp)
    elif os.path.basename(scene_name) == 'jogging':
        dummy_caps = []
        start_id = 67
        interval = 0.01
        for i in range(nframes):
            temp = copy.deepcopy(scene.captures[start_id])
            temp.cam_pose.camera_center_in_world -= interval * i * temp.cam_pose.right
            dummy_caps.append(temp)
    elif os.path.basename(scene_name) == 'lab':
        dummy_caps = []
        start_id = 39
        ellipse_a = 0.15 * 10
        ellipse_b = 0.03
        for i in range(nframes):
            temp = copy.deepcopy(scene.captures[start_id])
            x_offset= temp.cam_pose.right * (ellipse_a * np.cos(i/nframes * 2 * np.pi))
            y_offset= temp.cam_pose.up * ellipse_b * np.sin(i/nframes * 2 * np.pi)
            temp.cam_pose.camera_center_in_world = temp.cam_pose.camera_center_in_world + x_offset + y_offset
            temp.cam_pose.camera_center_in_world += temp.cam_pose.forward * 0.2
            dummy_caps.append(temp)
    return dummy_caps


class NeumanDataset(torch.utils.data.Dataset):
    
    #     | å‚æ•°å             | è¯´æ˜                                              |
    # | --------------- | ----------------------------------------------- |
    # | `seq`           | åºåˆ—åï¼Œå¦‚ `'seattle'`ï¼Œç”¨äºå®šä½æ•°æ®è·¯å¾„                      |
    # | `split`         | æ•°æ®åˆ’åˆ†ï¼š`'train'`, `'val'`, `'anim'`               |
    # | `render_mode`   | æ¸²æŸ“æ¨¡å¼ï¼Œæ§åˆ¶æ¸²æŸ“åŒºåŸŸï¼ˆä¾‹å¦‚ `human_scene`, `scene`, `human`ï¼‰ |
    # | `add_bg_points` | æ˜¯å¦æ·»åŠ èƒŒæ™¯çƒç‚¹äº‘ï¼ˆç”¨äºæ„å»ºå®Œæ•´çƒå£³ç¯å¢ƒï¼‰                           |
    def __init__(
        self, seq, split, 
        render_mode='human_scene',
        add_bg_points=False, 
        num_bg_points=204_800,
        bg_sphere_dist=5.0,
        clean_pcd=False,
    ):
        # dataset_path='data/neuman/dataset'
        # seq=lab
        dataset_path = f"{NEUMAN_PATH}/{seq}"

        # scene: åŒ…å«å›¾åƒå¸§ã€ç›¸æœºå‚æ•°ã€ç‚¹äº‘ã€å°ºå¯¸ä¿¡æ¯çš„å¯¹è±¡ï¼›
        scene = neuman_helper.NeuManReader.read_scene(
            dataset_path,
            tgt_size=None,
            normalize=False,
            smpl_type='optimized'
        )
        
        # smpl_params([
        #     'pose',          # (N, 72)
        #     'betas',         # (N, 10)
        #     'trans',         # (N, 3)
        #     'scale',         # float æˆ– shape=(1,)
        #     'gender'         # str æˆ– int (0, 1, 2)
        # ])
        smpl_params_path = f'{dataset_path}/4d_humans/smpl_optimized_aligned_scale.npz'        
        smpl_params = np.load(smpl_params_path)
        smpl_params = {f: smpl_params[f] for f in smpl_params.files}

        # ä¸ºäº† é©±åŠ¨åŠ¨ç”»ï¼ˆé©±åŠ¨ SMPL éª¨æ¶äº§ç”Ÿè¿ç»­åŠ¨ä½œåºåˆ—ï¼‰
        if split == 'anim':
            #             | å†…å®¹                                           | è¯´æ˜                                            |
            # | -------------------------------------------- | --------------------------------------------- |
            # | `'./data/SFU/0008/0008_ChaCha001_poses.npz'` | å­˜å‚¨ SMPL åŠ¨ä½œå‚æ•°ï¼ˆ`pose`ï¼‰çš„ `.npz` æ–‡ä»¶ï¼Œæè¿°çš„æ˜¯â€œChaChaèˆâ€ |
            # | `0`                                          | èµ·å§‹å¸§ç´¢å¼•ï¼Œä» MoCap æ–‡ä»¶ä¸­ç¬¬ 0 å¸§å¼€å§‹ä½¿ç”¨                    |
            # | `1000`                                       | ç»ˆæ­¢å¸§ç´¢å¼•ï¼Œä½¿ç”¨åˆ°ç¬¬ 1000 å¸§ï¼ˆä¸å«ï¼‰                         |
            # | `4`                                          | æ—¶é—´æ­¥é‡‡æ ·ç‡ï¼Œæ¯éš” 4 å¸§å–ä¸€å¸§ï¼Œå³ `0, 4, 8, ..., 996`        |
            # â‘  åŠ è½½åŠ¨ä½œæ•°æ®ï¼ˆMoCapï¼‰
            #     mocap_path(seq)ï¼šæ ¹æ®å½“å‰åºåˆ—åç§°ï¼ˆå¦‚ 'lab'ï¼‰è¿”å›æŒ‡å®šåŠ¨ä½œæ–‡ä»¶è·¯å¾„å’Œèµ·å§‹ã€ç»ˆæ­¢å¸§ã€é‡‡æ ·é—´éš”ã€‚
            #     np.load(motion_path)ï¼šåŠ è½½ .npz åŠ¨ä½œæ–‡ä»¶ï¼ŒåŒ…å«ï¼š
            #     poses: (N, 156) ç»´ï¼ŒSMPL-H æ ¼å¼çš„å…³èŠ‚è§’åº¦å‚æ•°
            #     trans: (N, 3)ï¼Œæ¯ä¸€å¸§çš„å¹³ç§»å‘é‡
            motion_path, start_idx, end_idx, skip = mocap_path(seq)
            motions = np.load(motion_path)
            
            # AMASS_SMPLH_TO_SMPL_JOINTSï¼šç´¢å¼•æ˜ å°„ï¼Œå°† AMASS / SMPL-H ä¸­çš„ 52 ç»´èº«ä½“ pose æ˜ å°„ä¸º SMPL æ‰€éœ€çš„ 23 ç»´èº«ä½“ poseï¼ˆæ¯ä¸ªå…³èŠ‚3è½´æ—‹è½¬ï¼Œå…± 23Ã—3=69 ç»´ï¼‰ã€‚
            poses = motions['poses'][start_idx:end_idx:skip, AMASS_SMPLH_TO_SMPL_JOINTS]
            transl = motions['trans'][start_idx:end_idx:skip]
            betas = smpl_params['betas'][0]
            smpl_params = {
                'global_orient': poses[:, :3],
                'body_pose': poses[:, 3:],
                'transl': transl,
                'scale': np.array([1.0] * poses.shape[0]),
                'betas': betas[None].repeat(poses.shape[0], 0)[:, :10],
            }

            # alignment(seq) æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°ï¼Œè¿”å›å½“å‰åºåˆ—æ‰‹åŠ¨è®¾ç½®çš„ï¼š
            #     å¹³ç§»ï¼ˆtranslationï¼‰
            #     æ—‹è½¬è§’åº¦ï¼ˆeulerï¼‰
            #     ç¼©æ”¾æ¯”ä¾‹
            #     è¿™äº›æ˜¯ä¸ºäº† å°†åŠ¨ä½œåæ ‡ç³»å¯¹é½åˆ°åœºæ™¯åæ ‡ç³»ã€‚
            manual_trans, manual_rot, manual_scale = alignment(seq)
            manual_rotmat = transformations.euler_matrix(*manual_rot)[:3, :3]
            self.manual_rotmat = torch.from_numpy(manual_rotmat).float().unsqueeze(0)
            self.manual_trans = torch.from_numpy(manual_trans).float().unsqueeze(0)
            self.manual_scale = torch.tensor([manual_scale]).float().unsqueeze(0)
            
            # ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿçš„ scene.capturesï¼Œæ¯ä¸€å¸§å¯¹åº”ä¸€å¸§åŠ¨ä½œï¼Œå°†æ¸²æŸ“å‡†å¤‡å¥½
            # å®é™…æ¸²æŸ“å¯èƒ½ä½¿ç”¨å›ºå®šè§†è§’æˆ–é¢„å®šä¹‰è·¯å¾„åˆæˆâ€œåŠ¨ç”»æ¼”ç¤ºâ€
            nframes = poses.shape[0]
            caps = rendering_caps(seq, nframes, scene)
            scene.captures = caps
        else:
            self.train_split, _, self.val_split = get_data_splits(scene)
        
        self.scene = scene

        # ä»ç‚¹äº‘ä¸­æå–å‰ä¸‰åˆ—æ•°æ®ï¼Œå³æ¯ä¸ªç‚¹çš„ 3D åæ ‡ (X, Y, Z)ã€‚
        pcd_xyz = self.scene.point_cloud[:, :3]
        # ä»ç‚¹äº‘ä¸­æå– RGB é¢œè‰²ä¿¡æ¯ï¼Œå³ç¬¬4åˆ°ç¬¬6åˆ—ã€‚
        pcd_col = self.scene.point_cloud[:, 3:6] / 255.

        # ğŸ§¼ 1. clean_pcd: æ¸…é™¤ç‚¹äº‘ç¦»ç¾¤ç‚¹
        # åˆ©ç”¨ Open3D æ‰§è¡Œç»Ÿè®¡ç¦»ç¾¤ç‚¹å‰”é™¤
        if clean_pcd:
            import open3d as o3d
            scene_pcd = o3d.geometry.PointCloud()
            scene_pcd.points = o3d.utility.Vector3dVector(pcd_xyz)

            logger.debug(f'Num points before outlier removal: {len(pcd_xyz)}')
            _, inlier_ind = scene_pcd.remove_statistical_outlier(nb_neighbors=100, std_ratio=0.5)

            print(f'Num points after outlier removal: {len(inlier_ind)}')
            pcd_xyz = pcd_xyz[inlier_ind]
            pcd_col = pcd_col[inlier_ind]

        # æ·»åŠ èƒŒæ™¯çƒé¢ç‚¹ï¼ˆç¨€ç–ç‚¹åŒ…å›´åœºæ™¯ï¼‰
        if add_bg_points:
            # find the scene center and size
            point_max_coordinate = np.max(pcd_xyz, axis=0)
            point_min_coordinate = np.min(pcd_col, axis=0)
            scene_center = (point_max_coordinate + point_min_coordinate) / 2
            scene_size = np.max(point_max_coordinate - point_min_coordinate)
            # build unit sphere points
            n_points = num_bg_points
            samples = np.arange(n_points)
            y = 1 - (samples / float(n_points - 1)) * 2  # y goes from 1 to -1
            radius = np.sqrt(1 - y * y)  # radius at y
            phi = math.pi * (math.sqrt(5.) - 1.)  # golden angle in radians
            theta = phi * samples  # golden angle increment
            x = np.cos(theta) * radius
            z = np.sin(theta) * radius
            unit_sphere_points = np.concatenate([x[:, None], y[:, None], z[:, None]], axis=1)
            # build background sphere
            bg_sphere_point_xyz = (unit_sphere_points * scene_size * bg_sphere_dist) + scene_center
            bg_sphere_point_rgb = np.asarray(np.random.random(bg_sphere_point_xyz.shape))
            # add background sphere to scene
            pcd_xyz = np.concatenate([pcd_xyz, bg_sphere_point_xyz], axis=0)
            pcd_col = np.concatenate([pcd_col, bg_sphere_point_rgb], axis=0)

            import open3d as o3d
            pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pcd_xyz))
            pcd.colors = o3d.utility.Vector3dVector(pcd_col)
            o3d.io.write_point_cloud(f'./output/{seq}_bg_sphere.ply', pcd)
            logger.debug(f"Added {len(bg_sphere_point_xyz)} background points, saved to output/{seq}_bg_sphere.ply")

        # pcd_xyz é€šå¸¸åŒ…å«çš„æ˜¯ å…¨åœºæ™¯çš„é™æ€ç‚¹äº‘ï¼ˆstatic scene point cloudï¼‰ï¼Œä¸åŒ…æ‹¬äººä½“çš„é«˜æ–¯ç‚¹ã€‚
        # äººä½“ç‚¹ï¼ˆå³åŠ¨æ€äººèº«ä½“å¯¹åº”çš„é«˜æ–¯ï¼‰æ˜¯å•ç‹¬å»ºæ¨¡çš„ï¼šå®ƒä»¬åˆå§‹æ”¾ç½®åœ¨ SMPL ç½‘æ ¼é¡¶ç‚¹ä½ç½®
        # 3. åˆå§‹åŒ– BasicPointCloud
        self.init_pcd = BasicPointCloud(
            points=pcd_xyz, 
            colors=pcd_col, 
            normals=np.zeros_like(pcd_xyz), 
            faces=None
        )

        # äººä½“ç‚¹ï¼ˆå³åŠ¨æ€äººèº«ä½“å¯¹åº”çš„é«˜æ–¯ï¼‰æ˜¯å•ç‹¬å»ºæ¨¡çš„ï¼šå®ƒä»¬åˆå§‹æ”¾ç½®åœ¨ SMPL ç½‘æ ¼é¡¶ç‚¹ä½ç½®
        # è¿™å‡ ä¸ªå‚æ•°æ˜¯ç”¨æ¥é©±åŠ¨ SMPL æ¨¡å‹çš„æ ‡å‡†è¾“å…¥é¡¹ï¼Œç”¨äºç”Ÿæˆä¸äººä½“å§¿æ€å’Œå½¢ä½“ç›¸å…³çš„3Dç½‘æ ¼ã€‚å®ƒä»¬çš„å«ä¹‰å¦‚ä¸‹ï¼š
        # å­—æ®µåŒ…æ‹¬ï¼š
        # | å‚æ•°å             | ç»´åº¦     | å«ä¹‰         | åŠŸèƒ½æè¿°                           |
        # | --------------- | ------ | ---------- | ------------------------------ |
        # | `global_orient` | N Ã— 3  | å…¨å±€æœå‘       | æ§åˆ¶æ•´ä¸ªèº«ä½“åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„æ—‹è½¬ï¼ˆè½´è§’æ ¼å¼ï¼‰          |
        # | `body_pose`     | N Ã— 69 | èº«ä½“å§¿æ€       | æ§åˆ¶23ä¸ªèº«ä½“å…³èŠ‚çš„å±€éƒ¨æ—‹è½¬ï¼ˆæ¯ä¸ªå…³èŠ‚3ç»´è½´è§’ï¼Œå…±69ç»´ï¼‰  |
        # | `transl`        | N Ã— 3  | å¹³ç§»å‘é‡       | æ§åˆ¶äººä½“åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„ä½ç½®                  |
        # | `betas`         | N Ã— 10 | å½¢çŠ¶å‚æ•°ï¼ˆä¸ªä½“ç‰¹å¾ï¼‰ | æ§åˆ¶äººä½“çš„ä¸ªæ€§åŒ–å½¢çŠ¶ï¼ˆèº«é«˜ã€èƒ–ç˜¦ã€æ¯”ä¾‹ç­‰ï¼‰ï¼Œæ¥è‡ªPCAä¸»æˆåˆ† |
        # | `scale`         | N Ã— 1  | å…¨å±€ç¼©æ”¾å› å­     | å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œç»Ÿä¸€çš„ç¼©æ”¾è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒæ•°æ®æˆ–åœºæ™¯      |
        self.smpl_params = {}
        for k in smpl_params.keys():
            self.smpl_params[k] = torch.from_numpy(smpl_params[k]).float()

        # 5. åŠ è½½ SAM åˆ†å‰²æ©ç 
        # æ¯å¸§å›¾åƒçš„äººä½“äºŒå€¼åˆ†å‰²ç»“æœï¼Œæ¥è‡ª Segment Anything Modelï¼ˆSAMï¼‰
        self.sam_mask_dir = f'{dataset_path}/4d_humans/sam_segmentations'
        self.msk_lists = sorted(glob.glob(f"{self.sam_mask_dir}/*.png"))

        # ğŸ“ 6. è®¡ç®—ç›¸æœºåˆ†å¸ƒçš„å¯¹è§’å°ºåº¦ï¼ˆç”¨äºç¡®å®šåœºæ™¯åŠå¾„ï¼‰
        # ğŸ” ç›®çš„ï¼šè®¡ç®—æ•´ä¸ªç›¸æœºé˜µåˆ—çš„ç©ºé—´èŒƒå›´ï¼Œå¹¶æ®æ­¤è®¾å®šä¸€ä¸ªåˆé€‚çš„æ¸²æŸ“åŠå¾„ï¼ˆself.radiusï¼‰
        _, diag = get_center_and_diag([cap.cam_pose.camera_center_in_world for cap in scene.captures])
        # è®¾ç½®ä¸€ä¸ªæ¯”åœºæ™¯å°ºåº¦ç¨å¤§çš„çƒå½¢åŠå¾„
        #     å¸¸ç”¨äºï¼š
        #     è§†è§’é‡‡æ ·èŒƒå›´é™åˆ¶ï¼ˆå¦‚çƒé¢è½¨è¿¹ï¼‰
        #     è™šæ‹Ÿç›¸æœºè½¨è¿¹ç”Ÿæˆ
        #     æ¸²æŸ“æ—¶é«˜æ–¯ç‚¹çš„å¯è§†å‰”é™¤èŒƒå›´
        self.radius = diag * 1.1
        
        # ğŸ“… 7. è®¾ç½®å¸§æ•°å’Œæ¨¡å¼
        self.split = split # train / val / anim
        self.mode = render_mode # render_mode='human_scene',
        self.num_frames = len(self.scene.captures)    

        self.cached_data = None
        if self.cached_data is None:
            self.load_data_to_cuda()

    def __len__(self):
        if self.split == "train":
            return len(self.train_split)
        elif self.split == "val":
            return len(self.val_split)
        elif self.split == "anim":
            return self.num_frames
    
    def get_single_item(self, i):
        
        if self.split == "train":
            idx = self.train_split[i]
        elif self.split == "val":
            idx = self.val_split[i]
        elif self.split == "anim":
            idx = i
        
        cap = self.scene.captures[idx]
        
        datum = {}
        if self.split in ['train', 'val']:
            img = cap.captured_image.image # cv2.imread(self.img_lists[idx])
            
            msk = cv2.imread(self.msk_lists[idx], cv2.IMREAD_GRAYSCALE) / 255
            if self.mode == 'scene':
                msk = cv2.dilate(msk, np.ones((20,20), np.uint8), msk, iterations=1)
            
            # get bbox from mask
            rows = np.any(msk, axis=0)
            cols = np.any(msk, axis=1)
            ymin, ymax = np.where(rows)[0][[0, -1]]
            xmin, xmax = np.where(cols)[0][[0, -1]]
            bbox = np.array([xmin, ymin, xmax, ymax])

            img = (img[..., :3] / 255).astype(np.float32)
            msk = msk.astype(np.float32)
            
            img = img.transpose(2, 0, 1)
            datum.update({
                "rgb": torch.from_numpy(img).float(),
                "mask": torch.from_numpy(msk).float(),
                "bbox": torch.from_numpy(bbox).float(),
            })
        
        K = cap.intrinsic_matrix
        width = cap.size[1]
        height = cap.size[0]
        
        fovx = 2 * np.arctan(width / (2 * K[0, 0]))
        fovy = 2 * np.arctan(height / (2 * K[1, 1]))
        # zfar = max(cap.far['human'], cap.near['bkg']) + 1.0
        # znear = min(cap.near['human'], cap.near['bkg'])
        zfar = 100.0 # max(zfar, 100.0)
        znear = 0.01 # min(znear, 0.01)
        
        world_view_transform = torch.from_numpy(cap.cam_pose.world_to_camera).T # torch.eye(4)
        c2w = torch.from_numpy(cap.cam_pose.camera_to_world)
        
        projection_matrix = get_projection_matrix(znear=znear, zfar=zfar, fovX=fovx, fovY=fovy).transpose(0,1)
        full_proj_transform = (world_view_transform.unsqueeze(0).bmm(projection_matrix.unsqueeze(0))).squeeze(0)
        camera_center = world_view_transform.inverse()[3, :3]
        cam_intrinsics = torch.from_numpy(cap.intrinsic_matrix).float()
        
        datum.update({
            "fovx": fovx,
            "fovy": fovy,
            "image_height": height,
            "image_width": width,
            "world_view_transform": world_view_transform,
            "c2w": c2w,
            "full_proj_transform": full_proj_transform,
            "camera_center": camera_center,
            "cam_intrinsics": cam_intrinsics,
            
            "betas": self.smpl_params["betas"][idx],
            "global_orient": self.smpl_params["global_orient"][idx],
            "body_pose": self.smpl_params["body_pose"][idx],
            "transl": self.smpl_params["transl"][idx],
            "smpl_scale": self.smpl_params["scale"][idx],
            "near": znear,
            "far": zfar,
        })
        
        if self.split == 'anim':
            datum.update({
                "manual_rotmat": self.manual_rotmat,
                "manual_scale": self.manual_scale,
                "manual_trans": self.manual_trans,
            })
        
        return datum
    
    def load_data_to_cuda(self):
        self.cached_data = []
        for i in tqdm(range(self.__len__())):
            datum = self.get_single_item(i)
            for k, v in datum.items():
                if isinstance(v, torch.Tensor):
                    datum[k] = v.to("cuda")
            self.cached_data.append(datum)
                
    def __getitem__(self, idx):
        if self.cached_data is None:
            return self.get_single_item(idx, is_src=True)
        else:
            return self.cached_data[idx]
